import tensorflow as tf

x = [1, 2, 3]
y = [1, 2, 3]

w = tf.Variable(tf.random_normal([1]), name = 'weight')
b = tf.Variable(tf.random_normal([1]), name = 'bias')

hypothesis = x * w + b

loss = tf.reduce_mean(tf.square(hypothesis - y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
train = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(loss), sess.run(w), sess.run(b))

# results
# ====================
# 0 8.504627 [-0.45926848] [0.2567734]
# 20 0.15024547 [0.57691425] [0.67035604]
# 40 0.067785956 [0.6889321] [0.6793819]
# 60 0.06094165 [0.71232414] [0.65131265]
# 80 0.05534253 [0.72667915] [0.6210708]
# 100 0.050262917 [0.7396038] [0.59191775]
# 120 0.045649588 [0.75184906] [0.56410307]
# 140 0.041459706 [0.76351196] [0.5375927]
# 160 0.03765438 [0.7746261] [0.5123278]
# 180 0.03419827 [0.7852178] [0.48825023]
# 200 0.031059423 [0.79531175] [0.4653043]
# 220 0.02820869 [0.80493134] [0.4434367]
# 240 0.025619544 [0.81409895] [0.4225968]
# 260 0.02326811 [0.8228356] [0.4027363]
# 280 0.021132471 [0.8311617] [0.38380918]
# 300 0.019192854 [0.8390965] [0.36577156]
# 320 0.017431261 [0.84665823] [0.34858173]
# 340 0.015831333 [0.8538648] [0.33219972]
# 360 0.014378292 [0.86073256] [0.31658757]
# 380 0.013058583 [0.86727774] [0.30170915]
# 400 0.01186001 [0.8735151] [0.28752983]
# 420 0.010771451 [0.87945944] [0.27401704]
# 440 0.009782818 [0.8851244] [0.2611392]
# 460 0.00888491 [0.8905232] [0.24886666]
# 480 0.008069415 [0.89566815] [0.23717082]
# 500 0.007328756 [0.9005714] [0.22602463]
# 520 0.0066560972 [0.9052442] [0.21540232]
# 540 0.00604518 [0.9096974] [0.20527914]
# 560 0.0054903296 [0.9139412] [0.19563179]
# 580 0.0049863956 [0.91798574] [0.18643782]
# 600 0.0045287297 [0.9218401] [0.17767587]
# 620 0.004113059 [0.9255133] [0.16932572]
# 640 0.0037355449 [0.929014] [0.161368]
# 660 0.0033926868 [0.93235004] [0.15378426]
# 680 0.003081287 [0.9355293] [0.146557]
# 700 0.0027984756 [0.9385592] [0.13966939]
# 720 0.0025416226 [0.94144666] [0.13310546]
# 740 0.0023083426 [0.9441984] [0.12684998]
# 760 0.0020964786 [0.9468209] [0.12088851]
# 780 0.0019040477 [0.94932026] [0.11520719]
# 800 0.0017292885 [0.951702] [0.10979284]
# 820 0.001570574 [0.95397174] [0.10463297]
# 840 0.0014264099 [0.9561349] [0.09971561]
# 860 0.0012954978 [0.95819634] [0.09502935]
# 880 0.0011765884 [0.96016103] [0.09056335]
# 900 0.0010685936 [0.96203333] [0.08630721]
# 920 0.0009705157 [0.9638176] [0.08225109]
# 940 0.0008814361 [0.96551806] [0.07838559]
# 960 0.0008005334 [0.9671386] [0.07470176]
# 980 0.00072705885 [0.9686829] [0.07119107]
# 1000 0.0006603308 [0.9701547] [0.06784538]
# 1020 0.00059972075 [0.9715573] [0.06465689]
# 1040 0.0005446744 [0.972894] [0.06161825]
# 1060 0.00049468194 [0.9741679] [0.05872242]
# 1080 0.0004492785 [0.975382] [0.05596268]
# 1100 0.00040804164 [0.9765389] [0.05333262]
# 1120 0.00037059028 [0.9776414] [0.0508262]
# 1140 0.0003365761 [0.97869223] [0.04843758]
# 1160 0.0003056865 [0.9796936] [0.04616123]
# 1180 0.00027762898 [0.9806479] [0.04399183]
# 1200 0.00025214558 [0.9815574] [0.04192435]
# 1220 0.00022900477 [0.98242414] [0.03995406]
# 1240 0.00020798454 [0.98325014] [0.03807638]
# 1260 0.00018889655 [0.9840373] [0.03628691]
# 1280 0.00017155653 [0.98478746] [0.03458158]
# 1300 0.00015581079 [0.9855025] [0.03295636]
# 1320 0.00014150968 [0.9861838] [0.03140753]
# 1340 0.00012852061 [0.9868331] [0.02993148]
# 1360 0.00011672457 [0.98745185] [0.02852482]
# 1380 0.00010601303 [0.9880416] [0.02718428]
# 1400 9.6281154e-05 [0.9886036] [0.02590672]
# 1420 8.744368e-05 [0.98913914] [0.0246892]
# 1440 7.941865e-05 [0.9896496] [0.02352888]
# 1460 7.21285e-05 [0.99013615] [0.02242309]
# 1480 6.5508655e-05 [0.9905997] [0.02136923]
# 1500 5.9494985e-05 [0.9910415] [0.02036492]
# 1520 5.403525e-05 [0.99146247] [0.01940783]
# 1540 4.907524e-05 [0.99186367] [0.01849574]
# 1560 4.4571545e-05 [0.99224603] [0.01762653]
# 1580 4.0479874e-05 [0.99261045] [0.01679815]
# 1600 3.6765086e-05 [0.9929578] [0.01600871]
# 1620 3.338977e-05 [0.99328876] [0.01525633]
# 1640 3.0325855e-05 [0.9936041] [0.01453932]
# 1660 2.7542044e-05 [0.9939047] [0.01385604]
# 1680 2.5014058e-05 [0.99419117] [0.01320486]
# 1700 2.2718736e-05 [0.9944641] [0.01258429]
# 1720 2.0633386e-05 [0.9947243] [0.01199289]
# 1740 1.8739558e-05 [0.9949722] [0.01142928]
# 1760 1.7019667e-05 [0.99520856] [0.01089216]
# 1780 1.5457155e-05 [0.99543375] [0.01038026]
# 1800 1.4038519e-05 [0.9956483] [0.00989242]
# 1820 1.2749934e-05 [0.9958529] [0.00942749]
# 1840 1.15796765e-05 [0.9960478] [0.00898441]
# 1860 1.05168465e-05 [0.9962335] [0.00856215]
# 1880 9.551489e-06 [0.9964105] [0.00815975]
# 1900 8.674862e-06 [0.9965792] [0.00777629]
# 1920 7.87873e-06 [0.9967399] [0.00741084]
# 1940 7.1554405e-06 [0.99689317] [0.00706258]
# 1960 6.499106e-06 [0.9970392] [0.00673066]
# 1980 5.902159e-06 [0.9971783] [0.00641433]
# 2000 5.3605904e-06 [0.99731094] [0.00611288]
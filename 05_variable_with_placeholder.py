import tensorflow as tf

x = tf.placeholder(tf.float32, shape = [None])
y = tf.placeholder(tf.float32, shape = [None])

w = tf.Variable(tf.random_normal([1]), name = 'weight')
b = tf.Variable(tf.random_normal([1]), name = 'bias')

hypothesis = x * w + b

loss = tf.reduce_mean(tf.square(hypothesis - y))

optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)
train = optimizer.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for step in range(2001):
    loss_val, w_val, b_val, _ = sess.run([loss, w, b, train], feed_dict = {x:[1, 2, 3], y:[1, 2, 3]})
    if step % 20 == 0:
        print(step, loss_val, w_val, b_val)

# results
# ====================
# 0 52.317234 [-1.3491786] [-1.439909]
# 20 0.48938084 [0.9048321] [-0.4292617]
# 40 0.01809237 [1.1134174] [-0.31929833]
# 60 0.0125659555 [1.127523] [-0.29574278]
# 80 0.011377566 [1.1233805] [-0.28102994]
# 100 0.010332981 [1.1177583] [-0.2677452]
# 120 0.009384568 [1.1122408] [-0.2551548]
# 140 0.008523219 [1.1069676] [-0.24316277]
# 160 0.007740918 [1.1019404] [-0.23173493]
# 180 0.0070304163 [1.0971497] [-0.22084413]
# 200 0.0063851424 [1.0925841] [-0.21046534]
# 220 0.005799092 [1.088233] [-0.20057425]
# 240 0.0052668364 [1.0840864] [-0.19114807]
# 260 0.0047834297 [1.0801345] [-0.18216488]
# 280 0.0043443874 [1.0763686] [-0.17360374]
# 300 0.0039456342 [1.0727795] [-0.16544497]
# 320 0.0035834843 [1.0693591] [-0.15766959]
# 340 0.00325458 [1.0660995] [-0.15025967]
# 360 0.002955867 [1.0629932] [-0.14319804]
# 380 0.0026845736 [1.0600327] [-0.13646841]
# 400 0.002438164 [1.0572114] [-0.13005486]
# 420 0.0022143817 [1.0545226] [-0.12394277]
# 440 0.0020111415 [1.0519603] [-0.11811793]
# 460 0.0018265463 [1.0495183] [-0.11256685]
# 480 0.0016588994 [1.0471911] [-0.10727661]
# 500 0.0015066416 [1.0449734] [-0.10223503]
# 520 0.0013683555 [1.0428598] [-0.09743036]
# 540 0.0012427603 [1.0408453] [-0.09285147]
# 560 0.0011286879 [1.0389256] [-0.08848762]
# 580 0.0010250966 [1.0370964] [-0.08432893]
# 600 0.00093100383 [1.035353] [-0.08036574]
# 620 0.00084555295 [1.0336915] [-0.07658882]
# 640 0.0007679425 [1.0321082] [-0.07298942]
# 660 0.000697456 [1.0305992] [-0.0695592]
# 680 0.0006334426 [1.0291611] [-0.06629013]
# 700 0.00057530316 [1.0277907] [-0.0631747]
# 720 0.0005224988 [1.0264846] [-0.06020576]
# 740 0.00047454177 [1.0252401] [-0.05737635]
# 760 0.00043098768 [1.0240538] [-0.05467992]
# 780 0.00039142824 [1.0229235] [-0.05211018]
# 800 0.00035550664 [1.021846] [-0.04966127]
# 820 0.00032287787 [1.0208195] [-0.04732748]
# 840 0.0002932417 [1.0198408] [-0.04510321]
# 860 0.00026632653 [1.0189084] [-0.04298347]
# 880 0.00024188044 [1.0180198] [-0.04096334]
# 900 0.00021967881 [1.0171729] [-0.03903822]
# 920 0.00019951642 [1.0163659] [-0.03720356]
# 940 0.00018120404 [1.0155967] [-0.03545513]
# 960 0.0001645722 [1.0148638] [-0.03378888]
# 980 0.00014946841 [1.0141653] [-0.03220095]
# 1000 0.00013574808 [1.0134995] [-0.03068758]
# 1020 0.00012328803 [1.0128651] [-0.02924535]
# 1040 0.00011197416 [1.0122606] [-0.02787096]
# 1060 0.00010169524 [1.0116843] [-0.02656112]
# 1080 9.236158e-05 [1.0111352] [-0.02531287]
# 1100 8.38857e-05 [1.0106119] [-0.0241233]
# 1120 7.618687e-05 [1.0101132] [-0.02298965]
# 1140 6.919282e-05 [1.0096378] [-0.02190921]
# 1160 6.28419e-05 [1.0091848] [-0.02087951]
# 1180 5.707491e-05 [1.0087533] [-0.01989823]
# 1200 5.1835097e-05 [1.0083419] [-0.01896308]
# 1220 4.7078338e-05 [1.0079498] [-0.01807191]
# 1240 4.2756223e-05 [1.0075762] [-0.01722259]
# 1260 3.883252e-05 [1.0072203] [-0.01641321]
# 1280 3.5269226e-05 [1.006881] [-0.01564188]
# 1300 3.203218e-05 [1.0065575] [-0.01490681]
# 1320 2.9091534e-05 [1.0062493] [-0.01420621]
# 1340 2.642037e-05 [1.0059556] [-0.01353853]
# 1360 2.3996517e-05 [1.0056757] [-0.01290226]
# 1380 2.1793321e-05 [1.005409] [-0.01229589]
# 1400 1.9793157e-05 [1.0051548] [-0.01171807]
# 1420 1.797689e-05 [1.0049125] [-0.01116739]
# 1440 1.6326123e-05 [1.0046816] [-0.01064252]
# 1460 1.4828173e-05 [1.0044618] [-0.01014239]
# 1480 1.3467293e-05 [1.0042521] [-0.00966579]
# 1500 1.2231297e-05 [1.0040522] [-0.00921154]
# 1520 1.1108984e-05 [1.0038618] [-0.00877865]
# 1540 1.0088889e-05 [1.0036802] [-0.00836611]
# 1560 9.163253e-06 [1.0035074] [-0.00797295]
# 1580 8.322243e-06 [1.0033424] [-0.00759825]
# 1600 7.558396e-06 [1.0031854] [-0.00724115]
# 1620 6.8644513e-06 [1.0030358] [-0.00690086]
# 1640 6.234368e-06 [1.0028931] [-0.00657658]
# 1660 5.662536e-06 [1.0027571] [-0.00626753]
# 1680 5.142652e-06 [1.0026276] [-0.00597301]
# 1700 4.671021e-06 [1.0025041] [-0.00569236]
# 1720 4.242098e-06 [1.0023865] [-0.00542485]
# 1740 3.85316e-06 [1.0022744] [-0.00516993]
# 1760 3.49937e-06 [1.0021675] [-0.00492701]
# 1780 3.1782595e-06 [1.0020657] [-0.00469553]
# 1800 2.8867562e-06 [1.0019686] [-0.00447493]
# 1820 2.6217588e-06 [1.0018761] [-0.00426465]
# 1840 2.3809773e-06 [1.001788] [-0.00406427]
# 1860 2.1624667e-06 [1.0017039] [-0.00387331]
# 1880 1.9640327e-06 [1.0016239] [-0.00369131]
# 1900 1.7838456e-06 [1.0015477] [-0.00351788]
# 1920 1.6202542e-06 [1.001475] [-0.00335267]
# 1940 1.4715056e-06 [1.0014056] [-0.00319516]
# 1960 1.3366808e-06 [1.0013397] [-0.00304506]
# 1980 1.2140694e-06 [1.0012767] [-0.00290203]
# 2000 1.1026192e-06 [1.0012167] [-0.00276574]